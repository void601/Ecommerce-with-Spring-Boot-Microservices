# Input section: Reads data from Kafka topic
input {
  kafka {
    # Specifies the Kafka broker (server) to connect to
    bootstrap_servers => "kafka1:29092"  # Corrected Kafka server address

    # Defines the Kafka topic from which Logstash will consume messages
    topics => ["application-logs"]

    # Specifies that messages in Kafka are in JSON format
    codec => json

    # Optional: Set a consumer group to avoid re-consuming the same messages
    group_id => "logstash-consumer-group"  # Specify a consumer group
  }
}

# Filter section: Used to process and transform incoming data (currently empty)
filter {
  # Add any additional filtering or transformations here
  # Example: Remove unnecessary fields, parse logs, or enrich data
}

# Output section: Sends processed data to Elasticsearch for indexing
output {
  elasticsearch {
    # Specifies the Elasticsearch server to send data to
    hosts => ["http://elasticsearch:9200"]  # Corrected Elasticsearch server address

    # Defines the index name pattern where logs will be stored
    # The %{+YYYY.MM.dd} part dynamically creates a new index each day (e.g., application-logs-2025.03.13)
    index => "application-logs-%{+YYYY.MM.dd}"
  }

  # Optional: Sends output to console for debugging (uncomment if needed)
  stdout { codec => rubydebug }
  
  # Optional: Error output to file for troubleshooting (uncomment if needed)
  # file {
  #   path => "/path/to/logstash_error.log"
  # }
}
